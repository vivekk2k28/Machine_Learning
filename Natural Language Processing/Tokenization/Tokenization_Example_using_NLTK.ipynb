{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAOvG/AA4lglKYFagvSAuy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivekk2k28/Machine_Learning/blob/main/Natural%20Language%20Processing/Tokenization/Tokenization_Example_using_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOe4kKlUcvyU",
        "outputId": "4fe36d2d-04e4-422c-acdb-66f65f59f1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# problem i'm getting when i run the \"sent_tokenize(corpus)\" statement in google colab so we need to download punkt."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bZRujeQfA1-",
        "outputId": "5eed06a0-4216-47f2-f5e8-a80d4f51474f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Hello Welcome, to VK's NLP Tutorials.\n",
        "Please do watch the entire course! to become expert in NLP.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hhfDsD_NdF_m"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IE1vBtpqdwbc",
        "outputId": "319f98a6-44e1-459d-a664-f27b71c86b33"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Welcome, to VK's NLP Tutorials.\\nPlease do watch the entire course! to become expert in NLP.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg3x-9ZBdyFE",
        "outputId": "190590ea-34d9-47d0-e17b-5d60a27c3dcd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome, to VK's NLP Tutorials.\n",
            "Please do watch the entire course! to become expert in NLP.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Sentence --> paragraph\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "ZxmKBNagd5YU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "77vTppazeJTl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS8Hgu9FeOdj",
        "outputId": "d16ff766-2882-46a9-922f-2923210ff74c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Hello Welcome, to VK's NLP Tutorials.\",\n",
              " 'Please do watch the entire course!',\n",
              " 'to become expert in NLP.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73SMSOrHfz0k",
        "outputId": "30a1b0e7-30a5-47a8-fa5a-ab1707de7ba0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUro0jLqf75a",
        "outputId": "44c58e45-4d32-4e6d-c51d-5a73ac110ce4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome, to VK's NLP Tutorials.\n",
            "Please do watch the entire course!\n",
            "to become expert in NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "## Paragraph --> words\n",
        "## sentence ---> words\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "fa7G2rWQgDwZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iwini4ASIRz",
        "outputId": "71e491e2-10e8-4d8c-e832-477a6b0c4406"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'VK',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-th_mat2SSB3",
        "outputId": "ad0434ca-aab0-4828-b40f-c459ed6e46b1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Welcome', ',', 'to', 'VK', \"'s\", 'NLP', 'Tutorials', '.']\n",
            "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
            "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "hCbuAsYNSxd_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)  # 's are splitted before it did'nt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AfdYA4HTImf",
        "outputId": "b1028675-73ee-4e68-ce4b-9a09a401d898"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'VK',\n",
              " \"'\",\n",
              " 's',\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "s5AvlSEmTM7-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer() # . are not separarted word here but last . are considered"
      ],
      "metadata": {
        "id": "PUkMI78hTpbG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2wX008lT1wQ",
        "outputId": "4ae98287-60ad-4931-becc-a03c99c1077c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'VK',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'Tutorials.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}